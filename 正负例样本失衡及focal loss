前段时间在解决分类任务时，发现当正负例比例相差较大时，分类算法更倾向于优化比例较大的类别的loss，最终导致正负例上的正确率有极大悬殊。
这其实是做分类任务时经常遇到的问题，即正负例比例不均衡，解决此类问题的办法除了较常用的over-samping/under-sampling之外，还有一类方法是修改loss函数，使其能更均衡的去学习正负样本。
比较容易想到的方法是直接对正负样本的loss加上不同的权重，比如正样本loss的权重为alpha，负样本的权重为(1 - alpha)，在我看来，这种方法本质上和over-sampling/under-sampling没有区别，针对的是所有的正样本或者所有的负样本去调整weight，粒度显得略大，因为有些正样本或者负样本是被正确分类的，它们的loss不应该被放大。
于是，何恺明大神在Focal Loss for Dense Object Detection这篇论文中提出使用focal loss替换分类问题中常用的cross entropy loss，由于本人主要专注于文本方向，并没有仔细看这篇图像相关的paper，但文中提出的focal loss是可以用在任何类别不均衡问题中的。
